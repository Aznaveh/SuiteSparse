\documentclass[12pt]{article}
\usepackage{hyperref}
\usepackage{minted}

\topmargin -0.5in
\textheight 9.0in
\oddsidemargin 0pt
\evensidemargin 0pt
\textwidth 6.5in

%-------------------------------------------------------------------------------
% get epsf.tex file, for encapsulated postscript files:
\input epsf
%-------------------------------------------------------------------------------
% macro for Postscript figures the easy way
% usage:  \postscript{file.ps}{scale}
% where scale is 1.0 for 100%, 0.5 for 50% reduction, etc.
%
\newcommand{\postscript}[2]
{\setlength{\epsfxsize}{#2\hsize}
\centerline{\epsfbox{#1}}}
%-------------------------------------------------------------------------------

\title{User's Guide for ParU, an unsymmetric multifrontal multithreaded sparse
LU factorization package}
\author{Mohsen Aznaveh\thanks{
email: aznaveh@tamu.edu.
http://www.suitesparse.com.
},
Timothy A. Davis}

\date{VERSION 0.0.0, May 20, 2022}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------
\maketitle

\begin{abstract}

ParU is an implementation of the multifrontal sparse LU factorization
method.  Parallelism is exploited both in the BLAS and across different frontal
matrices using OpenMP tasking, a shared-memory programming model for modern 
multicore architectures. The package is written in C++ and real sparse matrices 
are supported.

\end{abstract}

\maketitle

%-------------------------------------------------------------------------------
\section{Introduction}
\label{intro}
%-------------------------------------------------------------------------------

The algorithms used in ParU will be discussed in a companion paper,
?. This document gives detailed information on the installation
and use of ParU.
ParU is a parallel sparse direct solver. This package uses OpenMP
tasking for parallelism. ParU calls UMFPACK for symbolic analysis phase,
after that some symbolic analysis is done by ParU itself and  then numeric
phase starts. The numeric computation is a task parallel phase using OpenMP
and each task calls parallel BLAS; i.e. nested parallism. 
The performance of BLAS has a heavy impact on the performance of ParU.
However, depending on the input problem performance of parallelsim in BLAS 
sometimes does not have effects in ParU.


%-------------------------------------------------------------------------------
\subsubsection{Instructions on using METIS}
%-------------------------------------------------------------------------------

SuiteSparse now on METIS 5.1.0, which is distributed along with
SuiteSparse itself.  Its use is optional, however. ParU is using METIS as the 
default ordering. METIS tends to give orderings that are good for the 
parallelism. You can compile and run your code without using METIS; We recommend 
using METIS along with ParU.

Note that METIS is not bug-free; it can occasionally cause segmentation 
faults, particularly if used when finding basic solutions to underdetermined 
systems with many more columns than rows .ParU do not solve such 
systems anyway but you might see some problems with other SuiteSparse packages.

%-------------------------------------------------------------------------------
\section{Using ParU in C and C++}
%-------------------------------------------------------------------------------

ParU relies on CHOLMOD for its basic sparse matrix data structure, a compressed 
sparse column format.  CHOLMOD provides interfaces to the AMD, COLAMD, and METIS
ordering methods, writing a matrix to a file, and many other
functions. ParU also relies on UMFPACK Version 6.0 or higher for symbolic 
analysis. 


%-------------------------------------------------------------------------------
\subsection{Installing the C/C++ library on Linux/Unix}
%-------------------------------------------------------------------------------

Before you compile the ParU library and demo programs, you may wish to
edit the 

\verb'SuiteSparse/SuiteSparse_config/SuiteSparse_config.mk' 
configuration file.  The defaults should be fine on most Linux/Unix systems and 
on the Mac.
It automatically detects what system you have and sets compile parameters
accordingly.

The configuration file defines where the LAPACK and BLAS libraries are to be
found.  Selecting the right BLAS is critical.  There is no standard naming
scheme for the name and location of these libraries.  The defaults in the
\verb'SuiteSparse_config.mk' file use \verb'-llapack' and \verb'-lblas';
the latter may link against the standard Fortran reference BLAS, which will not 
provide optimal performance.  For best results, you should use the OpenBLAS
at openblas.net
(based on the Goto BLAS)
\cite{GotoVanDeGeijn08}, or high-performance vendor-supplied BLAS such as the
Intel MKL, AMD ACML, or the Sun Performance Library.  Selection of LAPACK and
the BLAS is done with the \verb'LAPACK=' and \verb'BLAS=' lines in the
\verb'SuiteSparse_config.mk' file.

There are two parts that are important in chosing the compiler and 
\verb'BLAS' library.


\verb 'AUTOCC ?= yes' This line let \verb'SuiteSparse_config' choose the 
compiler automitcally. If there is intel compiler avialable it will be chose. 
If you change \verb'yes' to \verb'no' then GCC will be used for the compilation.


\verb 'BLAS ?= -lopenblas' This line let \verb'SuiteSparse_config' choose the 
\verb'BLAS' library. By default ParU uses  \verb'openBLAS'. If you comment out
this line ParU will look for Intel Math Kernel library. 

After you decide about the compiler and \verb'BLAS' library, type \verb'make' at 
the Linux/Unix command line, in either the 
\verb'SuiteSparse' directory (which compiles all of SuiteSparse) or in the 
\verb'SuiteSparse/ParU' directory (which just compiles ParU and the 
libraries it requires)???.  ParU will be compiled, and a set of simple demos 
will be run (including the one in the next section).

To  test the lines of ParU, go to the \verb'Tcov'
directory and type \verb'make'.  To fully test 100\% of the lines of ParU you 
should define \verb'PARU_ALLOC_TESTING' and \verb'PARU_COVERAGE' in
\verb'ParU\Source\paru_internal.hpp'.
This will work for Linux only.

To install the shared library
into /usr/local/lib and /usr/local/include, do {\tt make install}.
To uninstall, do {\tt make uninstall}.
For more options, see the {\tt SuiteSparse/README.txt} file.

%-------------------------------------------------------------------------------
\subsection{C/C++ Example}
%-------------------------------------------------------------------------------

The C++ interface is written using only real matrices.  
The simplest function computes the MATLAB equivalent of
\verb'x=A\b' and is almost as simple:
Below is a simple C++ program that illustrates the use of ParU.  The
program reads in a problem from \verb'stdin' in MatrixMarket
format \cite{BoisvertPozoRemingtonBarrettDongarra97}, solves it, and prints the
norm of \verb'A' and the residual. 
Some error testing code is omited to only show how the program works. The full 
program can be found in 
\verb'Paru/Demo/paru_demo.cpp'
\begin{minted}{c}
#include "ParU.hpp"
int main(int argc, char **argv)
{
    cholmod_common Common, *cc;
    cholmod_sparse *A;
    ParU_Symbolic *Sym = NULL;

    //~~~~~~~~~Reading the input matrix and test if the format is OK~~~~~~~~~~~~
    // start CHOLMOD
    cc = &Common;
    int mtype;
    cholmod_l_start(cc);

    // A = mread (stdin) ; read in the sparse matrix A
    A = (cholmod_sparse *)cholmod_l_read_matrix(stdin, 1, &mtype, cc);
    //~~~~~~~~~~~~~~~~~~~Starting computation~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ParU_Control Control;
    ParU_Ret info;
    info = ParU_Analyze(A, &Sym, &Control);
    ParU_Numeric *Num;
    info = ParU_Factorize(A, Sym, &Num, &Control);
    double my_time = omp_get_wtime() - my_start_time;
    //~~~~~~~~~~~~~~~~~~~Test the results ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Int m = Sym->m;
    if (info == PARU_SUCCESS)
    {
        double *b = (double *)malloc(m * sizeof(double));
        double *xx = (double *)malloc(m * sizeof(double));
        for (Int i = 0; i < m; ++i) b[i] = i + 1;
        info = ParU_Solve(Sym, Num, b, xx, &Control);
        printf("Solve time is %lf seconds.\n", my_solve_time);
        double resid, anorm;
        info = ParU_Residual(A, xx, b, m, resid, anorm, &Control);
        printf("Residual is |%.2lf| and anorm is %.2e and rcond is %.2e.\n",
                resid == 0 ? 0 : log10(resid), anorm, Num->rcond);
        free(b);
        free(xx);
    }
    //~~~~~~~~~~~~~~~~~~~End computation~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Int max_threads = omp_get_max_threads();
    BLAS_set_num_threads(max_threads);

    //~~~~~~~~~~~~~~~~~~~Free Everything~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ParU_Freenum(&Num, &Control);
    ParU_Freesym(&Sym, &Control);

    cholmod_l_free_sparse(&A, cc);
    cholmod_l_finish(cc);
}
\end{minted}


%-------------------------------------------------------------------------------
\subsection{C++ Syntax}
%-------------------------------------------------------------------------------

The following is a list of user-callable C++ functions and what they
can do:

\begin{enumerate}

    \item \verb'ParU_Version': return the version of the ParU package 
        you are using.

    \item \verb'ParU_Analyze': Symbolic analysis is done in this routine. 
        UMFPACK is called here and after that some more speciallized symbolic
        computation is done for ParU. 
        \verb'ParU_Analyze' called once and used for differen 
        \verb'ParU_Factorize'
        calls.
    \item \verb'ParU_Factorize': 
        Numeric factorization is done in this routine. Scaling and
        making Sx matrix, computing factors and permutations is here. 
        \verb'ParU_Symbolic' structure is computed \verb'ParU_Analyze' 
        and is an input in this routine.

    \item \verb'ParU_Solve':  
        Using symbolic analysis and factorization phase output to solve $Ax=b$.
        In all the solve routines Num structure must come with the same 
        Sym struct that comes from \verb'ParU_Factorize'. 
        This routin is overloaded and can solve differently. It has versions 
        that keep a copy of x or overwrite on it. Also it can solve multiple 
        right hand side problems.

    \item \verb'ParU_Freenum':  frees the numerical part of factorization.


    \item \verb'ParU_Freesym':  frees the symbolic part of factorization.

\end{enumerate}

%-------------------------------------------------------------------------------
\subsection{Details of the C/C++ Syntax}
%-------------------------------------------------------------------------------

For further details of how to use the C/C++ syntax, please refer to the
definitions and descriptions in the following files:

\begin{enumerate}
\item \verb'SuiteSparse/ParU/Include/ParU.hpp' describes each
C++ function.  Only \verb'double' and square matrices are supported.


\item \verb'SuiteSparse/ParU/Include/ParU.h' describes
the C-callable functions.

\end{enumerate}

There are C/C++ options to control ParU which is an input argument to several 
routines. When you make \verb'ParU_Control' object it is initialized with 
default values. The user can change the values. Here are the list of control 
options:


Other parameters, such as \verb'opts.ordering' and \verb'opts.tol',
are input parameters to the various C/C++ functions.  Others such as
\verb"opts.solution='min2norm'" are separate functions in the C/C++
interface.  Refer to the files listed above for details.
Output statistics include:

\vspace{0.1in}
{\footnotesize
\begin{tabular}{|lll|}
\hline
    \verb'ParU_Control' & default value & explanation  \\
\hline\hline
\verb'mem_chunk' & $1024*1024$ & chunk size for memset and memcpy\\
\verb'umfpack_ordering' & \verb'UMFPACK_ORDERING_METIS' & default UMFPACK ordering\\
\verb'umfpack_strategy' & \verb'UMFPACK_STRATEGY_AUTO'& default UMFPACK strategy\\
\verb'relaxed_amalgamation_threshold' & 32 & threshold for relaxed amalgamation \\
\hline
\verb'scale' & 1 & if 1 matrix will be scaled using \verb'max_row'\\
\verb'panel_width' & 32 & width of panel for dense factorizaiton\\
\verb'paru_strategy' & \verb'PARU_STRATEGY_AUTO' & default strategy for ParU\\
\verb'piv_toler' & $0.1$ & tolerance for accepting sparse pivots\\
\verb'diag_toler' & $0.001$ & tolerance for accepting symmetric pivots\\
\verb'trivial' & $4$ & Do not call BLAS for smaller dgemms\\
\verb'worthwhile_dgemm' & $512$ & dgemms bigger than worthwhile are tasked\\
\verb'worthwhile_trsm' & $4096$ & trsm bigger than worthwhile are tasked\\
\verb'paru_max_threads' & $0$ & initialized with \verb'omp_max_threads' \\
\hline
\end{tabular}
}
\vspace{0.1in}

The first row of the options are used in symbolic analysis. In the symbolic 
analysis phase only the pattern of the matrix and numerical values are not 
probed. The second row have impact on numerical analysis.

\verb'paru_max_threads' is initalized by \verb'omp_max_threads' if the user do 
not provide a smaller number.
If \verb'paru_strategy' is set to \verb'PARU_STRATEGY_AUTO' ParU uses the same 
strategy as UMFPACK, however user can ask UMFPACK for a unsymmteric strategy 
but use a symmteric strategy for ParU.
    
%-------------------------------------------------------------------------------
\section{Requirements and Availability}
\label{summary}
%-------------------------------------------------------------------------------

ParU requires several Collected Algorithms of the ACM: CHOLMOD
\cite{ChenDavisHagerRajamanickam09,DavisHager09} (version 1.7 or later), AMD
\cite{AmestoyDavisDuff96,AmestoyDavisDuff03}, COLAMD
\cite{DavisGilbertLarimoreNg00_algo,DavisGilbertLarimoreNg00} and UMFPACK 
\cite{10.1145/992200.992206} for its
ordering/analysis phase and for its basic sparse matrix data structure, and the
BLAS \cite{dddh:90} for dense matrix computations on its frontal matrices. 
An efficient implementation of the BLAS is strongly recommended, either
vendor-provided (such as the Intel MKL, the AMD ACML, or the 
Sun Performance Library) or other high-performance BLAS such as those of 
\cite{GotoVanDeGeijn08}. Note that while ParU uses nested parallelism heavily
the right options for BLAS library must be chosen.

The use of Intel's Threading Building Blocks is optional \cite{Reinders07}, but
without it, only parallelism within the BLAS can be exploited (if available).
Suite\-SparseQR can optionally use METIS 4.0.1 \cite{KarypisKumar98e} and two
constrained minimum degree ordering algorithms, CCOLAMD and CAMD
\cite{ChenDavisHagerRajamanickam09}, for its fill-reducing ordering options.
SuiteSparseQR can be compiled without these ordering methods and without TBB.

In addition to appearing as Collected Algorithm 8xx of the ACM, SuiteSparseQR
is available at
\htmladdnormallink{http://www.suitesparse.com}{http://www.suitesparse.com}
and at MATLAB Central
in the user-contributed File Exchange (
\htmladdnormallink{http://www.mathworks.com/matlabcentral}{http://www.mathworks.com/matlabcentral}
).
See SPQR/Doc/License.txt for the license.
Alternative licenses are also
available; contact the author for details.

%-------------------------------------------------------------------------------
% References
%-------------------------------------------------------------------------------

\bibliographystyle{plain}
\bibliography{paru_user_guide}
\end{document}

